{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8114fe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/deepset-ai/haystack.git\n",
      "  Cloning https://github.com/deepset-ai/haystack.git to c:\\users\\flavio\\appdata\\local\\temp\\pip-req-build-7fafrhj3\n",
      "  Resolved https://github.com/deepset-ai/haystack.git to commit c38943721fbd702367a8934cb660a72b3143eb86\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: boilerpy3 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (1.0.6)\n",
      "Collecting canals==0.4.0 (from farm-haystack==1.20.0rc0)\n",
      "  Obtaining dependency information for canals==0.4.0 from https://files.pythonhosted.org/packages/b2/38/d91124721f0a6885fafee64352cd6dd6ba1450d6dd86258a59c551424f05/canals-0.4.0-py3-none-any.whl.metadata\n",
      "  Using cached canals-0.4.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: events in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (0.4)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (4.17.3)\n",
      "Collecting lazy-imports==0.3.1 (from farm-haystack==1.20.0rc0)\n",
      "  Using cached lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (9.1.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (3.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (2.0.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (9.5.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (3.9.1)\n",
      "Requirement already satisfied: posthog in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (3.0.1)\n",
      "Collecting prompthub-py==4.0.0 (from farm-haystack==1.20.0rc0)\n",
      "  Obtaining dependency information for prompthub-py==4.0.0 from https://files.pythonhosted.org/packages/27/5f/8c4939e290ff93af79364b88ffe3902d29c234f94e8227cf0b7fce3c887f/prompthub_py-4.0.0-py3-none-any.whl.metadata\n",
      "  Using cached prompthub_py-4.0.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: pydantic<2 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (1.10.7)\n",
      "Requirement already satisfied: quantulum3 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (0.9.0)\n",
      "Requirement already satisfied: rank-bm25 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (0.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (2.30.0)\n",
      "Requirement already satisfied: requests-cache<1.0.0 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (0.9.8)\n",
      "Collecting safetensors<0.3.2 (from farm-haystack==1.20.0rc0)\n",
      "  Using cached safetensors-0.3.1-cp38-cp38-win_amd64.whl (263 kB)\n",
      "Collecting scikit-learn>=1.3.0 (from farm-haystack==1.20.0rc0)\n",
      "  Obtaining dependency information for scikit-learn>=1.3.0 from https://files.pythonhosted.org/packages/5f/08/c66e99f06fb73f727c870172f0962c103262ac68839cc05234709b7b45c2/scikit_learn-1.3.0-cp38-cp38-win_amd64.whl.metadata\n",
      "  Using cached scikit_learn-1.3.0-cp38-cp38-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sseclient-py in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (1.7.2)\n",
      "Requirement already satisfied: tenacity in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (8.2.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.2 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (0.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from farm-haystack==1.20.0rc0) (4.65.0)\n",
      "Collecting transformers==4.31.0 (from farm-haystack==1.20.0rc0)\n",
      "  Obtaining dependency information for transformers==4.31.0 from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=6.0 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from prompthub-py==4.0.0->farm-haystack==1.20.0rc0) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.31.0->farm-haystack==1.20.0rc0) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.31.0->farm-haystack==1.20.0rc0)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.31.0->farm-haystack==1.20.0rc0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.31.0->farm-haystack==1.20.0rc0) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.31.0->farm-haystack==1.20.0rc0) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.31.0->farm-haystack==1.20.0rc0) (0.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pydantic<2->farm-haystack==1.20.0rc0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->farm-haystack==1.20.0rc0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->farm-haystack==1.20.0rc0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->farm-haystack==1.20.0rc0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->farm-haystack==1.20.0rc0) (2022.12.7)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests-cache<1.0.0->farm-haystack==1.20.0rc0) (1.4.4)\n",
      "Requirement already satisfied: attrs>=21.2 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests-cache<1.0.0->farm-haystack==1.20.0rc0) (23.1.0)\n",
      "Requirement already satisfied: cattrs>=22.2 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests-cache<1.0.0->farm-haystack==1.20.0rc0) (22.2.0)\n",
      "Requirement already satisfied: url-normalize>=1.4 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests-cache<1.0.0->farm-haystack==1.20.0rc0) (1.4.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn>=1.3.0->farm-haystack==1.20.0rc0) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn>=1.3.0->farm-haystack==1.20.0rc0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn>=1.3.0->farm-haystack==1.20.0rc0) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm->farm-haystack==1.20.0rc0) (0.4.6)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jsonschema->farm-haystack==1.20.0rc0) (5.12.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jsonschema->farm-haystack==1.20.0rc0) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jsonschema->farm-haystack==1.20.0rc0) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->farm-haystack==1.20.0rc0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->farm-haystack==1.20.0rc0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->farm-haystack==1.20.0rc0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from posthog->farm-haystack==1.20.0rc0) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from posthog->farm-haystack==1.20.0rc0) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from posthog->farm-haystack==1.20.0rc0) (2.2.1)\n",
      "Requirement already satisfied: inflect in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from quantulum3->farm-haystack==1.20.0rc0) (6.0.4)\n",
      "Requirement already satisfied: num2words in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from quantulum3->farm-haystack==1.20.0rc0) (0.5.12)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cattrs>=22.2->requests-cache<1.0.0->farm-haystack==1.20.0rc0) (1.1.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->farm-haystack==1.20.0rc0) (2023.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from importlib-resources>=1.4.0->jsonschema->farm-haystack==1.20.0rc0) (3.15.0)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\flavio\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from num2words->quantulum3->farm-haystack==1.20.0rc0) (0.6.2)\n",
      "Using cached canals-0.4.0-py3-none-any.whl (33 kB)\n",
      "Using cached prompthub_py-4.0.0-py3-none-any.whl (6.9 kB)\n",
      "Using cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "Using cached scikit_learn-1.3.0-cp38-cp38-win_amd64.whl (9.2 MB)\n",
      "Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Building wheels for collected packages: farm-haystack\n",
      "  Building wheel for farm-haystack (pyproject.toml): started\n",
      "  Building wheel for farm-haystack (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for farm-haystack: filename=farm_haystack-1.20.0rc0-py3-none-any.whl size=787355 sha256=2eebd6e5e8b3f88eb8218dabbffb508e4f4793cd72dbb442591ee42f6299f0c3\n",
      "  Stored in directory: C:\\Users\\Flavio\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-nui9u_1e\\wheels\\97\\0d\\ae\\77cde17929fbf66c8320f19b30789acfe52e2312bb1d125be1\n",
      "Successfully built farm-haystack\n",
      "Installing collected packages: safetensors, lazy-imports, canals, scikit-learn, prompthub-py, huggingface-hub, transformers, farm-haystack\n",
      "  Attempting uninstall: canals\n",
      "    Found existing installation: canals 0.1.2\n",
      "    Uninstalling canals-0.1.2:\n",
      "      Successfully uninstalled canals-0.1.2\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.8\n",
      "    Uninstalling huggingface-hub-0.0.8:\n",
      "      Successfully uninstalled huggingface-hub-0.0.8\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.6.1\n",
      "    Uninstalling transformers-4.6.1:\n",
      "      Successfully uninstalled transformers-4.6.1\n",
      "  Attempting uninstall: farm-haystack\n",
      "    Found existing installation: farm-haystack 1.17.0rc0\n",
      "    Uninstalling farm-haystack-1.17.0rc0:\n",
      "      Successfully uninstalled farm-haystack-1.17.0rc0\n",
      "Successfully installed canals-0.4.0 farm-haystack-1.20.0rc0 huggingface-hub-0.16.4 lazy-imports-0.3.1 prompthub-py-4.0.0 safetensors-0.3.1 scikit-learn-1.3.0 transformers-4.31.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack.git 'C:\\Users\\Flavio\\AppData\\Local\\Temp\\pip-req-build-7fafrhj3'\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "farm 0.8.0 requires torch<1.9,>1.5, but you have torch 2.0.1+cu117 which is incompatible.\n",
      "farm 0.8.0 requires transformers==4.6.1, but you have transformers 4.31.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/deepset-ai/haystack.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790501b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import Dict, Union, Any, List, Optional, Tuple\n",
    "\n",
    "from haystack import MultiLabel, Document\n",
    "from haystack.pipelines import Pipeline\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.nodes import BM25Retriever\n",
    "from haystack.nodes.base import BaseComponent\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from unicodedata import normalize\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce8c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_query = \"dados-conle-anonimizado-assunto-notnull - dados-conle-anonimizado-assunto-notnull.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3be0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSLP_S:\n",
    "    def __plural_reduction(self, word):\n",
    "        excep = [\"lápis\",\"cais\",\"mais\",\"crúcis\",\"biquínis\",\"pois\",\"depois\",\"dois\",\"leis\" ]\n",
    "        excep_s = [\"aliás\",\"pires\",\"lápis\",\"cais\",\"mais\",\"mas\",\"menos\", \"férias\",\"fezes\",\"pêsames\",\"crúcis\",\"gás\", \"atrás\",\"moisés\",\"através\",\"convés\",\"ês\", \"país\",\"após\",\"ambas\",\"ambos\",\"messias\"]\n",
    "\n",
    "        len_word = len(word)\n",
    "        new_word = list(word)\n",
    "\n",
    "        if len_word >= 3:\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'n':\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'õ':\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return  sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'ã':\n",
    "                if word == 'mães':\n",
    "                    word = word[:-1]\n",
    "                    return word\n",
    "                else:\n",
    "                    new_word[-2] = 'o'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    sing = sing[:-1]\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'a':\n",
    "                if word != 'cais' and word != 'mais':\n",
    "                    new_word[-2] = 'l'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    sing = sing[:-1]\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'é':\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'e':\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'ó':\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i':\n",
    "                if word not in excep:\n",
    "                    new_word[-1] = 'l'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'l':\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'r':\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "\n",
    "            if new_word[-1] == 's':\n",
    "                if word not in excep_s:\n",
    "                    word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        word = self.__plural_reduction(word)\n",
    "\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1da4b4",
   "metadata": {},
   "source": [
    "# Base de dados câmara dos deputados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c647da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assunto= pd.read_table(caminho_query, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43732db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_assunto = df_assunto.to_numpy()\n",
    "y,X = arr_assunto[:,0],arr_assunto[:,1]\n",
    "y = [i.strip() for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83fac529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar(y,top_n):\n",
    "    if y in top_n:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25bca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliacaoRecall(isPreprocess,top_k=20):\n",
    "    quant_encontrado=0\n",
    "    quant_relevante =0\n",
    "    antes = time.time()\n",
    "    for l,x in zip(y,X):\n",
    "        \n",
    "        tokenized_query3 = x                   \n",
    "        if isPreprocess:\n",
    "            tokenized_query3 = preprocess(x)                   \n",
    "    \n",
    "    \n",
    "        top_n_stem_l = pipeline.run(query=tokenized_query3,params={\"Retriever\": {\"top_k\": top_k}})\n",
    "    \n",
    "\n",
    "        top_n = [top_n_stem_l['documents'][d].meta['name'].strip() for d in range(len(top_n_stem_l['documents']))]              #L\n",
    "    \n",
    "        quant_relevante+=1\n",
    "        quant_encontrado+=verificar(l,top_n)\n",
    "    \n",
    "    recall = quant_encontrado / quant_relevante\n",
    "    depois = time.time()\n",
    "    duracao = depois - antes\n",
    "    print(\"Recall: \"+str(recall))\n",
    "    print(\"Duração: %f\" %(duracao))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d71ecc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliacaoRR(isPreprocess,top_k=20):\n",
    "    \n",
    "    quant_encontrado=0\n",
    "    \n",
    "    for l,x in zip(y,X):\n",
    "        \n",
    "        tokenized_query3 = x                   \n",
    "        if isPreprocess:\n",
    "            tokenized_query3 = preprocess(x)                   \n",
    "    \n",
    "        top_n_stem_l = pipeline.run(query=tokenized_query3,params={\"Retriever\": {\"top_k\": top_k}})\n",
    "        top_n = [top_n_stem_l['documents'][d].meta['name'].strip() for d in range(len(top_n_stem_l['documents']))]              #L\n",
    "        if l.strip() == top_n[0]:\n",
    "            quant_encontrado+=1\n",
    "    rr = quant_encontrado / 295\n",
    "    print(\"RR: %f\" %(rr))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aae4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliacaoPrecision(isPreprocess,top_k=20):\n",
    "    soma=0\n",
    "    for l,x in zip(y,X):\n",
    "        quant_encontrado=0\n",
    "        tokenized_query3 = x                   \n",
    "        if isPreprocess:\n",
    "            tokenized_query3 = preprocess(x)                   \n",
    "       \n",
    "        top_n_stem_l = pipeline.run(query=tokenized_query3,params={\"Retriever\": {\"top_k\": top_k}})\n",
    "        top_n = [top_n_stem_l['documents'][d].meta['name'].strip() for d in range(len(top_n_stem_l['documents']))]              #L\n",
    "        \n",
    "        quant_encontrado=verificar(l,top_n)\n",
    "        \n",
    "        soma+=(quant_encontrado/top_k)\n",
    "        \n",
    "    rr = soma / 295\n",
    "    print(\"Precision: %f\" %(rr))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b88f982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliacaoMAP(isPreprocess,top_k=20):\n",
    "    l_v = list()\n",
    "    labels_nsL = list()\n",
    "    for l,x in zip(y,X):\n",
    "        \n",
    "        tokenized_query3 = x                   \n",
    "        if isPreprocess:\n",
    "            tokenized_query3 = preprocess(x)                   \n",
    "    \n",
    "        top_n_stem_l = pipeline.run(query=tokenized_query3,params={\"Retriever\": {\"top_k\": top_k}})\n",
    "        top_n = [top_n_stem_l['documents'][d].meta['name'].strip() for d in range(len(top_n_stem_l['documents']))]              #L\n",
    "        \n",
    "        l_v.append(l)\n",
    "        labels_nsL.append(top_n)\n",
    "        \n",
    "    denominador = 0\n",
    "    encontrou=False\n",
    "    soma=0\n",
    "    for Y,x in zip(l_v,labels_nsL):\n",
    "        calc = 0\n",
    "        quant=0\n",
    "        for k in x:\n",
    "            if str(k).strip() == Y.strip():\n",
    "                quant=1\n",
    "                encontrou=True\n",
    "            denominador+=1\n",
    "            calc=quant/denominador\n",
    "            if encontrou:\n",
    "                calc = calc * 1\n",
    "            else:\n",
    "                calc = calc * 0\n",
    "            soma+=calc\n",
    "            encontrou = False\n",
    "    MAP = soma /295\n",
    "    print(\"MAP@ %d: %f\" % (top_k,MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2566df",
   "metadata": {},
   "source": [
    "# ImgTeorPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944da510",
   "metadata": {},
   "source": [
    "# Pré processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19166942",
   "metadata": {},
   "source": [
    "## 1- Sem pré processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc7539d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"sem_preprocessamento_img\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "076f8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cdb17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97648462",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea1ba0",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b859a",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbaf6331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6101694915254238\n",
      "Duração: 11.110809\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a8663e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6677966101694915\n",
      "Duração: 10.396010\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a3badb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7016949152542373\n",
      "Duração: 11.523417\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a16207",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b5f52d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.122034\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de1ad3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.066780\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1e1c1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.035085\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(False,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e080371",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2fb7b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.004146\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bffd4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.003050\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bbbe00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.002415\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(False,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e337254",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b6cc7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.389831\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(False,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c868b",
   "metadata": {},
   "source": [
    "## 5- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45f91e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c56d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [word for word in terms if word not in stopwords]\n",
    "        terms = \" \".join(terms)\n",
    "        return terms\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9723e665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_5_img\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f728a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d8ab8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da86abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4aaa5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacfb3cc",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6e87e",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9cef4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6135593220338983\n",
      "Duração: 6.319998\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2230bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6644067796610169\n",
      "Duração: 8.899968\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8309c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7050847457627119\n",
      "Duração: 8.763332\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99461f37",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "352fbd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.122712\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2337f60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.066441\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2241991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.035254\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a54883",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6811381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.003567\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0495732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.002488\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7661170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.001845\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc5d33",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c565ff71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.379661\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7de317",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587fec55",
   "metadata": {},
   "source": [
    "## 8- Stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a2cef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Savoy:\n",
    "\n",
    "    def __removeAllPTAccent(self, old_word):\n",
    "        word = list(old_word)\n",
    "        len_word = len(word)-1\n",
    "        for i in range(len_word, -1, -1):\n",
    "            if word[i] == 'ä':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'â':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'à':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'á':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ã':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ê':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'é':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'è':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ë':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ï':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'î':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ì':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'í':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ü':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ú':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ù':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'û':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ô':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ö':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ó':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ò':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'õ':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ç':\n",
    "                word[i] = 'c'\n",
    "\n",
    "        new_word = \"\".join(word)\n",
    "        return new_word\n",
    "\n",
    "    def __finalVowelPortuguese(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 'e' or word[-1] == 'a' or word[-1] == 'o':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __remove_PTsuffix(self, word):\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and (word[-3] == 'r' or word[-3] == 's' or word[-3] == 'z' or word[-3] == 'l'):\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if (word[-1] == 's' and word[-2] == 'i') and (word[-3] == 'e' or word[-3] == 'é'):\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'ó':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                return sing\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'õ':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'ã':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-1] == 'e' and word[-2] == 't' and word[-3] == 'n' and word[-4] == 'e' and word[-5] == 'm':\n",
    "                word = word[:-5]\n",
    "                return word\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __normFemininPortuguese(self, word):\n",
    "\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word < 3 or word[-1] != 'a':\n",
    "            return word\n",
    "\n",
    "        if len_word > 6:\n",
    "\n",
    "            if word[-2] == 'h' and word[-3] == 'n' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'a' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'i' and word[-4] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-2] == 'n' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'o':\n",
    "                word = word[:-1]\n",
    "                return word\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ê'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'v' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'm' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 2:\n",
    "            word = self.__remove_PTsuffix(word)\n",
    "            word = self.__normFemininPortuguese(word)\n",
    "            word = self.__finalVowelPortuguese(word)\n",
    "            word = self.__removeAllPTAccent(word)\n",
    "\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3416b7",
   "metadata": {},
   "source": [
    "## 9- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f35f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed5cc837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc6ebc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_8_img\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97a0ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f33ca15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b80ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "011b262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de3aa6",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0bfab",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c537963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4745762711864407\n",
      "Duração: 10.940616\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ae9957e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5491525423728814\n",
      "Duração: 11.929490\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b1dfda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6135593220338983\n",
      "Duração: 16.984783\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d2136",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54d7afa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.094915\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd4f98ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.054915\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37d4e545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.030678\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fb69e",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e74e237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001685\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5e82a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001176\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb52cd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000648\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db458c9",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6b4173f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.301695\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723b9b5c",
   "metadata": {},
   "source": [
    "## 11- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c1e4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9fd74207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95940543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_9_img\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6c58a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43232224",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6373a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a57d0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f57ff",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57b5dd",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93f1d4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5457627118644067\n",
      "Duração: 6.604143\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bd40001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5830508474576271\n",
      "Duração: 9.431793\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e5494904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6576271186440678\n",
      "Duração: 12.889215\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db20a4",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38e50c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.109153\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10433bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.058305\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "73bcd1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.032881\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a174a86",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "306a57b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.002251\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0e3e6db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001243\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6ee47e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.001020\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318daa0",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c3e8ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.345763\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760fe05f",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + RSLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f9f64",
   "metadata": {},
   "source": [
    "## 20- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "04344d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1a939b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = _remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \"_\" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c42f683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    "  document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_18_img\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1cda5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "88d38eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0a56257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e2a64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0333082",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fccd35",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d733859b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5627118644067797\n",
      "Duração: 12.123640\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "962f9931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6\n",
      "Duração: 14.499931\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c8d859df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6542372881355932\n",
      "Duração: 24.119757\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b82b3d0",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5d2f21cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.112542\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5814118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.060000\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7c01a2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.032712\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdffa26",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "58867e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.002245\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "36d8da87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001324\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "42049932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000723\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ce5bc",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1b1274f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.345763\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c46c1c",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + Savoy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3815b42",
   "metadata": {},
   "source": [
    "## 26- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (Savoy) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "76f9b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "35b58bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = _remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \"_\" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7762ca7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_21_img\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11c460ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6735014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b865f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cb4e14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25eb11e",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979d09c",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d380d0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5830508474576271\n",
      "Duração: 7.161255\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ebf28784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6338983050847458\n",
      "Duração: 7.579061\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f00292a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6779661016949152\n",
      "Duração: 8.480478\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f4087",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "518f0de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.116610\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "89ce019c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.063390\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a9b72ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.033898\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53fc76",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5b172eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.002145\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "18ef2eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001354\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1dcb42d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000735\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c62dc65",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5cb06b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.372881\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22346dd",
   "metadata": {},
   "source": [
    "## 22- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "180937f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "922b3c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "39298a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_22_img\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7ff06342",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4933392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "60c46120",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2a0e3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd738e6",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6056625",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "695c0c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6169491525423729\n",
      "Duração: 5.740044\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f64c4a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6508474576271186\n",
      "Duração: 6.135203\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "abd4748b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7084745762711865\n",
      "Duração: 6.809492\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8176614",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d0863004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.123390\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "879e9971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.065085\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "56484425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.035424\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99258b",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "185d2139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.003034\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bb9ea9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001988\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e95200a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.001387\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a0c25",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e70eb824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.372881\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b65fd92",
   "metadata": {},
   "source": [
    "## 23- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLPS) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ab01b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e3e4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = _remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \"_\" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cb5bc605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_23_img\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "333dcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1ca988f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "381acb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "111ff66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012c1c8",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93f934",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4ba1eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6\n",
      "Duração: 7.385643\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ece5c2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6508474576271186\n",
      "Duração: 8.071139\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "15b10e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6915254237288135\n",
      "Duração: 8.881361\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98f170",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e48e4e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.120000\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "36ef2774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.065085\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3d258e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.034576\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e4b4e",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "73509edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.002188\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9c9c9960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001315\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cd2ebcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000695\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3cbb90",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "98dec2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.386441\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e31ae1",
   "metadata": {},
   "source": [
    "# Indexacão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc624f14",
   "metadata": {},
   "source": [
    "# Pré processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79476c40",
   "metadata": {},
   "source": [
    "## 1- Sem pré processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fde39f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"sem_preprocessamento_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "601cd767",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bb27f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9eb33a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda51215",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095f025",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ae20fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.2135593220338983\n",
      "Duração: 2.350755\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d749a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.2677966101694915\n",
      "Duração: 2.501412\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "99690e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.30847457627118646\n",
      "Duração: 2.766187\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d43e3",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "37b1fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.042712\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "049a2e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.026780\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f1bd2ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.015424\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(False,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d06152",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "61969117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.000430\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fb407ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.000404\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7f213218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000252\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(False,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce306df8",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bacb528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.111864\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(False,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f6f23",
   "metadata": {},
   "source": [
    "## 5- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2e17d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "629fc8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [word for word in terms if word not in stopwords]\n",
    "        terms = \" \".join(terms)\n",
    "        return terms\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c3affd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_5_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "71f9ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4cceff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e61582a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9037b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e040cd2c",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306a6de",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "25dd3a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.31864406779661014\n",
      "Duração: 1.578219\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c5785fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.3525423728813559\n",
      "Duração: 1.698661\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5250cf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.42033898305084744\n",
      "Duração: 1.913376\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7aa6ab",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7233bae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.063729\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5e82718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.035254\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9d102cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.021017\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb6aec",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fcacb261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001034\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5c8d18e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.000687\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0e2cf72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000406\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38556a4",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5d3eab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.169492\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05ecfd",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab7e4b",
   "metadata": {},
   "source": [
    "## 8- Stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "010d50ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Savoy:\n",
    "\n",
    "    def __removeAllPTAccent(self, old_word):\n",
    "        word = list(old_word)\n",
    "        len_word = len(word)-1\n",
    "        for i in range(len_word, -1, -1):\n",
    "            if word[i] == 'ä':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'â':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'à':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'á':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ã':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ê':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'é':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'è':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ë':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ï':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'î':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ì':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'í':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ü':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ú':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ù':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'û':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ô':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ö':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ó':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ò':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'õ':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ç':\n",
    "                word[i] = 'c'\n",
    "\n",
    "        new_word = \"\".join(word)\n",
    "        return new_word\n",
    "\n",
    "    def __finalVowelPortuguese(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 'e' or word[-1] == 'a' or word[-1] == 'o':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __remove_PTsuffix(self, word):\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and (word[-3] == 'r' or word[-3] == 's' or word[-3] == 'z' or word[-3] == 'l'):\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if (word[-1] == 's' and word[-2] == 'i') and (word[-3] == 'e' or word[-3] == 'é'):\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'ó':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                return sing\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'õ':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'ã':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-1] == 'e' and word[-2] == 't' and word[-3] == 'n' and word[-4] == 'e' and word[-5] == 'm':\n",
    "                word = word[:-5]\n",
    "                return word\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __normFemininPortuguese(self, word):\n",
    "\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word < 3 or word[-1] != 'a':\n",
    "            return word\n",
    "\n",
    "        if len_word > 6:\n",
    "\n",
    "            if word[-2] == 'h' and word[-3] == 'n' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'a' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'i' and word[-4] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-2] == 'n' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'o':\n",
    "                word = word[:-1]\n",
    "                return word\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ê'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'v' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'm' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 2:\n",
    "            word = self.__remove_PTsuffix(word)\n",
    "            word = self.__normFemininPortuguese(word)\n",
    "            word = self.__finalVowelPortuguese(word)\n",
    "            word = self.__removeAllPTAccent(word)\n",
    "\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da78cd9d",
   "metadata": {},
   "source": [
    "## 9- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "980b4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e8cac21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "622d8e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_8_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b05cef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "35f77f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f4a0b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0cc6cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ea4c0",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad985d",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b9db1f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.29152542372881357\n",
      "Duração: 2.726175\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d7ec8377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.34915254237288135\n",
      "Duração: 2.944405\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a4ff9132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4033898305084746\n",
      "Duração: 3.034096\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b660d35",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1ec6e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.058305\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "24f23671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.034915\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4b2adb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.020169\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e8904",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4015b7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001218\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6ff9752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.000719\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9ae3262b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000443\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99dd312",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "874444c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.176271\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2201653c",
   "metadata": {},
   "source": [
    "## 11- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3bebc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "805f34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9c779afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_9_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b8478f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8ea8eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c977067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "dbec45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e5447",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318266e",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "606c67a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.33220338983050846\n",
      "Duração: 1.761130\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ac348bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.38305084745762713\n",
      "Duração: 1.942361\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1e3d88d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4440677966101695\n",
      "Duração: 2.460926\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c176d23",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ff4cf4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.066441\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2f8d1878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.038305\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8fd230c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.022203\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78d9e8",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "927ef6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001058\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1625704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.000751\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d538fe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000463\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10934fd",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bee64290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.189831\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecad823",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + RSLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600543a",
   "metadata": {},
   "source": [
    "## 20- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c7a2af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4e9c297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = _remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \"_\" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "70d3ab13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    "  document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_18_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a8772ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9e032227",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "42e41926",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f820478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3ccc8",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6e2d3",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8d8a16fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.28135593220338984\n",
      "Duração: 3.904474\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b3e1d7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.3254237288135593\n",
      "Duração: 5.641844\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0bb6d8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.3898305084745763\n",
      "Duração: 4.219683\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6bd3e9",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9697c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.056271\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "cdd8ee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.032542\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "dce8fa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.019492\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4daa617",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4d7756f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001126\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8ac4c71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.000652\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "fa92c9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000398\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea35641",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f4f30051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.159322\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83693dc",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + Savoy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbcd8d",
   "metadata": {},
   "source": [
    "## 26- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (Savoy) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "99906d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "250ba583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = _remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \"_\" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2c9d4bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_21_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9f7d201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1664a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2a0d937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "35dcec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9d0c2",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d639c9",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "1959b89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.27796610169491526\n",
      "Duração: 2.631889\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "10fadc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.3423728813559322\n",
      "Duração: 2.990520\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "90cad4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4101694915254237\n",
      "Duração: 3.567929\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a6001",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c89143ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.055593\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f5bcbe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.034237\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0f49f9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.020508\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349b663",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "0df4dd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.000812\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1c67df58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.000524\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "db329ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000378\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688dd57",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f4b78530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.152542\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca94ae84",
   "metadata": {},
   "source": [
    "## 22- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "8e72cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c0fb26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "f0dda3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_22_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "dba162fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a5ab304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c15caa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1c46ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541fd89",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c89d4",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "248e357b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4\n",
      "Duração: 1.717355\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "6329dd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4406779661016949\n",
      "Duração: 1.826344\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "3c1bfa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5050847457627119\n",
      "Duração: 2.049564\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126239c",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "975eeec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.080000\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "be14e9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.044068\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "5bf91aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.025254\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d074c08",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "0a6072af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001214\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8b4a7e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.000858\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "331c7e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000481\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b70bde",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "85c8f50f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.230508\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23091c16",
   "metadata": {},
   "source": [
    "## 23- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLPS) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "84823f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "5afb713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = _remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \"_\" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ebf9c51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_23_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "fda8eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7f1b4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b27237ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "0aaee56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be821c",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66deec46",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7d40e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.34915254237288135\n",
      "Duração: 2.245774\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "2a4d622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4033898305084746\n",
      "Duração: 2.491656\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "014a01c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.45084745762711864\n",
      "Duração: 3.311332\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57024318",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "96597a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.069831\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e9efd426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.040339\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ef71ea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.022542\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d6e705",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2934671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001060\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "cd9cf51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.000731\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "534fb485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000446\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e8801",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "7bdec824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.183051\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b251ae45",
   "metadata": {},
   "source": [
    "# Emenda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84563c39",
   "metadata": {},
   "source": [
    "# Pré processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce2e76",
   "metadata": {},
   "source": [
    "## 1- Sem pré processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d832d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"sem_preprocessamento_emenda\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2c66f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b5dc5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "fa2a92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5f2de",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b4d69",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2c5157f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.3864406779661017\n",
      "Duração: 3.666599\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "64866b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.43728813559322033\n",
      "Duração: 5.541443\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "bb664959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4711864406779661\n",
      "Duração: 4.338825\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360e7fd",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d96a47cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.077288\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "765f6bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.043729\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "3ccfdf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.023559\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(False,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eec8d6",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4bdd2d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001689\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "fde5f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.000939\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "69acad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000493\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(False,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5129b409",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "2f1b3621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.244068\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(False,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3693efa",
   "metadata": {},
   "source": [
    "## 5- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "77086f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a87d5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [word for word in terms if word not in stopwords]\n",
    "        terms = \" \".join(terms)\n",
    "        return terms\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "cfb776ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_5_emenda\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "6c5b33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ef7b1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "6f9130e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "763d65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4269c9",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866c915",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "50cc24b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4\n",
      "Duração: 1.696531\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "9b9b35b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4406779661016949\n",
      "Duração: 1.790010\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "957ecb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.488135593220339\n",
      "Duração: 1.992247\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a0cbf",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "833b8423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.080000\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "59012732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.044068\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "358d1c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.024407\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee04ec",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "a5835a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001732\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "23570d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.000951\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "92a169be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000500\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8ecaa5",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "70126303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.267797\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcbb76c",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f24d51",
   "metadata": {},
   "source": [
    "## 8- Stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "ffc6a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Savoy:\n",
    "\n",
    "    def __removeAllPTAccent(self, old_word):\n",
    "        word = list(old_word)\n",
    "        len_word = len(word)-1\n",
    "        for i in range(len_word, -1, -1):\n",
    "            if word[i] == 'ä':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'â':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'à':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'á':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ã':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ê':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'é':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'è':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ë':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ï':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'î':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ì':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'í':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ü':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ú':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ù':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'û':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ô':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ö':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ó':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ò':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'õ':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ç':\n",
    "                word[i] = 'c'\n",
    "\n",
    "        new_word = \"\".join(word)\n",
    "        return new_word\n",
    "\n",
    "    def __finalVowelPortuguese(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 'e' or word[-1] == 'a' or word[-1] == 'o':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __remove_PTsuffix(self, word):\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and (word[-3] == 'r' or word[-3] == 's' or word[-3] == 'z' or word[-3] == 'l'):\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if (word[-1] == 's' and word[-2] == 'i') and (word[-3] == 'e' or word[-3] == 'é'):\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'ó':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                return sing\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'õ':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'ã':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-1] == 'e' and word[-2] == 't' and word[-3] == 'n' and word[-4] == 'e' and word[-5] == 'm':\n",
    "                word = word[:-5]\n",
    "                return word\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __normFemininPortuguese(self, word):\n",
    "\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word < 3 or word[-1] != 'a':\n",
    "            return word\n",
    "\n",
    "        if len_word > 6:\n",
    "\n",
    "            if word[-2] == 'h' and word[-3] == 'n' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'a' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'i' and word[-4] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-2] == 'n' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'o':\n",
    "                word = word[:-1]\n",
    "                return word\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ê'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'v' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'm' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 2:\n",
    "            word = self.__remove_PTsuffix(word)\n",
    "            word = self.__normFemininPortuguese(word)\n",
    "            word = self.__finalVowelPortuguese(word)\n",
    "            word = self.__removeAllPTAccent(word)\n",
    "\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe83ed",
   "metadata": {},
   "source": [
    "## 9- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "29e16f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "bedb0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "d13d07a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_8_emenda\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "224bdf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "250ef38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "842e6f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "c6640720",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bdb32d",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401889c",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "61282785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.3694915254237288\n",
      "Duração: 2.788079\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "951aed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4135593220338983\n",
      "Duração: 2.926431\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "0f664623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4610169491525424\n",
      "Duração: 3.081398\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfed86",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "f9e3fc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.073898\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "0345b319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.041356\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "b2549109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.023051\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa36811a",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "90d005a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001774\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "08b641bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.000962\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "5a2e5c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000525\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0cdea3",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "27b67640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.227119\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6afb79f",
   "metadata": {},
   "source": [
    "## 11- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "83483ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "1a5159bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "584c5e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_9_emenda\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "773bfc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "9dc84481",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "402687d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "fcfc1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296f5f9",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68691983",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "c92d9f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.39322033898305087\n",
      "Duração: 1.847597\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "449275b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.44745762711864406\n",
      "Duração: 1.964634\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "7c94cee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4711864406779661\n",
      "Duração: 2.306371\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37886da5",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "4443066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.078644\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "a9c98493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.044746\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "4e2a6f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.023559\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174bfa5",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "38692510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001828\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "ebd34aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001017\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "1ca5d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000537\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7da490",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "70cc95fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.261017\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd10fa0",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + RSLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df362df6",
   "metadata": {},
   "source": [
    "## 20- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "5c18fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "2a8589e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = _remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \"_\" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "12f1dffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    "document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_18_emenda\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "e9a45ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "d4fe7ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "f56dd344",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "b6e2e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349418a",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b79ce6b",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "231e44fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.38305084745762713\n",
      "Duração: 4.195381\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "46cb7e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.43050847457627117\n",
      "Duração: 4.440234\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "9ba53b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4847457627118644\n",
      "Duração: 4.466233\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66602cd6",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "245fd864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.076610\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "00bb0bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.043051\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "c0bb401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.024237\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467bf283",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "6a81adb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001833\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "bce38da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001011\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "63c642ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000566\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92286d26",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "7cf6f7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.244068\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9126a3d1",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + Savoy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5235b3",
   "metadata": {},
   "source": [
    "## 26- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (Savoy) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "18ca3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "0d5ceb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = _remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \"_\" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "72b61ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_21_emenda\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "9c8118b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "c70fbda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "592fbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "c2985894",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba778da",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ede98",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "28affafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.39661016949152544\n",
      "Duração: 2.797016\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "c5ec1771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.43389830508474575\n",
      "Duração: 2.866767\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "36e60eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.48135593220338985\n",
      "Duração: 3.547949\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ef4eb",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "272c9f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.079322\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "5426aee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.043390\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "afaf388e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.024068\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982a246",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "247a4d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001758\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "24205e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.000978\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "bf477141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000540\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52035c",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "e3be1453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.261017\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141bd350",
   "metadata": {},
   "source": [
    "## 22- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "6faa7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "d2b8294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "65046a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_22_emenda\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "5d2204f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "a244b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "8a84e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "266b7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3fdc63",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4333afc",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "f0e11877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4271186440677966\n",
      "Duração: 1.772321\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "2c6db061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.488135593220339\n",
      "Duração: 1.899911\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "52b73e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5186440677966102\n",
      "Duração: 2.125612\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13272d00",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "59d97a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.085424\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "c54bd096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.048814\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "3d8b005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.025932\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291a8d4",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "476d0c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001772\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "2f3ba155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001055\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "a9ce594e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000554\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6e2c7",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "996f5092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.281356\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62762a72",
   "metadata": {},
   "source": [
    "## 23- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLPS) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "eea396cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "867132ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = _remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \"_\" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "2c10f893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_23_emenda\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "11dc152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "1a7982a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "9d3b20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "144c1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56870682",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65c9f9",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "268a8f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.423728813559322\n",
      "Duração: 2.455121\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "3a1ddf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4711864406779661\n",
      "Duração: 2.646664\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "bf2dad9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5084745762711864\n",
      "Duração: 3.285854\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a2e61",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "f08edb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.084746\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "a714da27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.047119\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "08a9b361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.025424\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac95a98",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "b456ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001762\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "0849ce53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001032\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "a222ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000543\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019b538",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "4c18987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.277966\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571f2d2",
   "metadata": {},
   "source": [
    "# Emenda + Indexação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c2b16",
   "metadata": {},
   "source": [
    "# Pré processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd0d5a",
   "metadata": {},
   "source": [
    "## 1- Sem pré processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "b2f4eefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"sem_preprocessamento_emenda_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "76981ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "11c9cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "89ded881",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e6b4e",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e9f9f2",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "a8be81a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.43050847457627117\n",
      "Duração: 4.094975\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "15b334e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5016949152542373\n",
      "Duração: 4.364391\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "929b86a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5491525423728814\n",
      "Duração: 4.827554\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84a6ab",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "d78c85e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.086102\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "7da7333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.050169\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "7b12e200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.027458\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(False,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435581b7",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "4c9a0fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001758\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "4cf5b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001104\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(False,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "85a31de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000621\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(False,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f3e3b8",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "07d52587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.244068\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(False,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56fdd5",
   "metadata": {},
   "source": [
    "## 5- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "249a0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "a1585b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [word for word in terms if word not in stopwords]\n",
    "        terms = \" \".join(terms)\n",
    "        return terms\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "1f42368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_5_emenda_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "fdad1c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "ea000a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "8491cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "35c422e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b4574",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09092739",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "0aae102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4406779661016949\n",
      "Duração: 1.937387\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "49af309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5220338983050847\n",
      "Duração: 2.078382\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "3f986738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5898305084745763\n",
      "Duração: 2.343803\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233792e3",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "89031ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.088136\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "a46e65d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.052203\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "f09c159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.029492\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98a63c",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "b71ff452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001782\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "ba0f8467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001042\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "89e671e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000659\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071f6d5",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "a3b04311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.261017\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f588a1",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c758e736",
   "metadata": {},
   "source": [
    "## 8- Stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "2ec1fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Savoy:\n",
    "\n",
    "    def __removeAllPTAccent(self, old_word):\n",
    "        word = list(old_word)\n",
    "        len_word = len(word)-1\n",
    "        for i in range(len_word, -1, -1):\n",
    "            if word[i] == 'ä':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'â':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'à':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'á':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ã':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ê':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'é':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'è':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ë':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ï':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'î':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ì':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'í':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ü':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ú':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ù':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'û':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ô':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ö':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ó':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ò':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'õ':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ç':\n",
    "                word[i] = 'c'\n",
    "\n",
    "        new_word = \"\".join(word)\n",
    "        return new_word\n",
    "\n",
    "    def __finalVowelPortuguese(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 'e' or word[-1] == 'a' or word[-1] == 'o':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __remove_PTsuffix(self, word):\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and (word[-3] == 'r' or word[-3] == 's' or word[-3] == 'z' or word[-3] == 'l'):\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if (word[-1] == 's' and word[-2] == 'i') and (word[-3] == 'e' or word[-3] == 'é'):\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'ó':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                return sing\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'õ':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'ã':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-1] == 'e' and word[-2] == 't' and word[-3] == 'n' and word[-4] == 'e' and word[-5] == 'm':\n",
    "                word = word[:-5]\n",
    "                return word\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __normFemininPortuguese(self, word):\n",
    "\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word < 3 or word[-1] != 'a':\n",
    "            return word\n",
    "\n",
    "        if len_word > 6:\n",
    "\n",
    "            if word[-2] == 'h' and word[-3] == 'n' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'a' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'i' and word[-4] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-2] == 'n' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'o':\n",
    "                word = word[:-1]\n",
    "                return word\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ê'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'v' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'm' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 2:\n",
    "            word = self.__remove_PTsuffix(word)\n",
    "            word = self.__normFemininPortuguese(word)\n",
    "            word = self.__finalVowelPortuguese(word)\n",
    "            word = self.__removeAllPTAccent(word)\n",
    "\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931aefcf",
   "metadata": {},
   "source": [
    "## 9- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "fadd205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "af516f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "82ad51e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    "document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_8_emenda_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "49c8414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "794b5192",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "9b8ea33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "6c8dbc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9680871",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0ee0b",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "a569e712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4033898305084746\n",
      "Duração: 3.315403\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "ff95228b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4610169491525424\n",
      "Duração: 3.266070\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "ba50a379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5220338983050847\n",
      "Duração: 3.551873\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f649f86",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "f546d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.080678\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "2ca07f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.046102\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "7cb83b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.026102\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012618d2",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "2302aed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001844\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "bcc93d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001203\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "e3025061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000646\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc55376",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "bbf54b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.264407\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfc05d",
   "metadata": {},
   "source": [
    "## 11- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "451cb871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "3d4bd5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "3fe5a40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_9_emenda_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "82490f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "ff8e2fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "a2ebd32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "eefa28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb15bdb",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce6ca33",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "92d3005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4406779661016949\n",
      "Duração: 2.150806\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "9a37b59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4847457627118644\n",
      "Duração: 2.327866\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "d05e4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.535593220338983\n",
      "Duração: 2.717614\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971a186",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "188f4e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.088136\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "d66d818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.048475\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "09baed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.026780\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca827e",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "1dc337d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001916\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "2ce5a433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001101\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "b760eb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000642\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf06194",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "8fc99c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.288136\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f421bb",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + RSLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1df06",
   "metadata": {},
   "source": [
    "## 20- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "451253e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "a2ce90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = _remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \"_\" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "f85319ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    "  document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_18_emenda_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "c595bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "24ec64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "3cfe3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "1fe93b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646d2fd",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb49985b",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "0dad4437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.41694915254237286\n",
      "Duração: 4.795602\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "26d20a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4542372881355932\n",
      "Duração: 5.081614\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "a3da7568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5084745762711864\n",
      "Duração: 5.015390\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44b52d",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "6e0a6c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.083390\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "19a498e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.045424\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "0861618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.025424\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f11c2a",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "ca93e865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.002011\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "a4c41446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001074\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "ea8764a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000646\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3442d9",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "bea04bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.250847\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2bede",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + Savoy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37936bef",
   "metadata": {},
   "source": [
    "## 26- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (Savoy) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "534be2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "70fcc4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = _remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \"_\" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "080eaf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_21_emenda_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "e6de528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "5a8a95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "2d1a87d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "f1ee512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ec9e7",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea412d",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "ad58a1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.43050847457627117\n",
      "Duração: 3.115163\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "c229147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.48135593220338985\n",
      "Duração: 3.836871\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "9330fad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5186440677966102\n",
      "Duração: 3.971284\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27d5192",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "14cdb06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.086102\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "419fe88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.048136\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "03f4d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.025932\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f5d30",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "f1ce919c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001909\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "5ef00355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001062\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "3edecd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000566\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe860d",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "23c862ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.267797\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f5e8b",
   "metadata": {},
   "source": [
    "## 22- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "3dad88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "6e95bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "f0f6bf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_22_emenda_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "79721c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "0ccf47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "203430f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "f10a152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a542cdad",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b1a15",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "4ba864ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.488135593220339\n",
      "Duração: 2.062332\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "15d5282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5627118644067797\n",
      "Duração: 2.212214\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "78187466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5932203389830508\n",
      "Duração: 2.482816\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5b0ff",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "a7e45a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.097627\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "74d95277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.056271\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "f5bcb793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.029661\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f8ca2",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "41b4226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.002071\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "60a1d723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001163\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "6e6cfed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000691\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e6685c",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "2ccbd3ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.288136\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdafc14",
   "metadata": {},
   "source": [
    "## 23- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLPS) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "bd36760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "c9530b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = str(txt)\n",
    "        txt = _remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \"_\" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "d81b1d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This ElasticsearchDocumentStore has been built for Elasticsearch 7, but the detected version of the Elasticsearch server is 8.8.2. Unexpected behaviors or errors may occur due to version incompatibility.\n"
     ]
    }
   ],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='TlHbFKwz9YMAprROSrgA', index=\"preprocessamento_23_emenda_indexacao\",\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "a834a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "5dc90147",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "8c10384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "ed18ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd17cb",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bd6e64",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "d86f81e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4711864406779661\n",
      "Duração: 2.712744\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "36ff5e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5084745762711864\n",
      "Duração: 3.020604\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "5e29dd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5694915254237288\n",
      "Duração: 4.029001\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf25114",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "48092646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.094237\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "71af491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.050847\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "fa5f4304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.028475\n"
     ]
    }
   ],
   "source": [
    "avaliacaoPrecision(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058d3cd8",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "6607d44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 5: 0.001887\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "279a5882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 10: 0.001082\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "a9dd94a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@ 20: 0.000661\n"
     ]
    }
   ],
   "source": [
    "avaliacaoMAP(True,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a24ff9",
   "metadata": {},
   "source": [
    "### RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "4fc530ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR: 0.281356\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRR(True,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
