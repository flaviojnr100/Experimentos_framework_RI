{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0cf84a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Flavio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Flavio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\Flavio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from unicodedata import normalize\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('rslp')\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50615748",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6187de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc027dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codProposicao</th>\n",
       "      <th>txtSiglaTipo</th>\n",
       "      <th>numAno</th>\n",
       "      <th>numNumero</th>\n",
       "      <th>txtNome</th>\n",
       "      <th>txtEmenta</th>\n",
       "      <th>txtExplicacaoEmenta</th>\n",
       "      <th>txtIndexacao</th>\n",
       "      <th>txtInteiroTeor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16357</td>\n",
       "      <td>PL</td>\n",
       "      <td>1999</td>\n",
       "      <td>1165</td>\n",
       "      <td>PL 1165/1999</td>\n",
       "      <td>Altera dispositivo da Lei nº 8.987, de 13 de f...</td>\n",
       "      <td>Estabelece que as concessionárias disponibiliz...</td>\n",
       "      <td>Alteração, Lei das Concessões de Serviços Públ...</td>\n",
       "      <td>Ofício nº 1416  (SF)                          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19098</td>\n",
       "      <td>PL</td>\n",
       "      <td>1992</td>\n",
       "      <td>3097</td>\n",
       "      <td>PL 3097/1992</td>\n",
       "      <td>Dispõe sobre a eleição de diretores de fundos ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NORMAS, ELEIÇÃO DIRETA, EMPREGADO, APOSENTADO,...</td>\n",
       "      <td>COMISSÃO DE CONSTITUIÇÃO E JUSTIÇA E DE REDAÇÃ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20464</td>\n",
       "      <td>PL</td>\n",
       "      <td>2000</td>\n",
       "      <td>3927</td>\n",
       "      <td>PL 3927/2000</td>\n",
       "      <td>Altera a composição dos Tribunais Regionais do...</td>\n",
       "      <td>Altera a composição do TRT da 5ª região, 6ª re...</td>\n",
       "      <td>Alteração, Lei Federal, composição, Tribunal R...</td>\n",
       "      <td>COMISSÃO DE TRABALHO, DE ADMINISTRAÇÃO E SERVI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20683</td>\n",
       "      <td>PL</td>\n",
       "      <td>1998</td>\n",
       "      <td>4117</td>\n",
       "      <td>PL 4117/1998</td>\n",
       "      <td>Dispõe sobre o acesso a ambientes de uso colet...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Autorização, pessoa portadora de deficiência, ...</td>\n",
       "      <td>Câmara dos Deputados\\n           Departamento ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20857</td>\n",
       "      <td>PL</td>\n",
       "      <td>1998</td>\n",
       "      <td>4395</td>\n",
       "      <td>PL 4395/1998</td>\n",
       "      <td>Estabelece as Diretrizes Nacionais de Defesa C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fixação, diretrizes, defesa civil, (Sindec), d...</td>\n",
       "      <td>- 1 - \\nCOMISSÃO DE CONSTITUIÇÃO E JUSTIÇA E D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57104</th>\n",
       "      <td>2358873</td>\n",
       "      <td>PL</td>\n",
       "      <td>2023</td>\n",
       "      <td>2233</td>\n",
       "      <td>PL 2233/2023</td>\n",
       "      <td>Altera o parágrafo único do art. 71 do Código ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 \\n  \\n \\n \\n \\nCÂMARA DOS DEPUTADOS \\nPROJET...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57105</th>\n",
       "      <td>2358874</td>\n",
       "      <td>PL</td>\n",
       "      <td>2019</td>\n",
       "      <td>3616</td>\n",
       "      <td>PL 3616/2019</td>\n",
       "      <td>Altera a Lei nº 9.503, de 23 de setembro de 19...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Altera a Lei nº 9.503, de 23 de setembro\\nde 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57106</th>\n",
       "      <td>2358875</td>\n",
       "      <td>PL</td>\n",
       "      <td>2019</td>\n",
       "      <td>1822</td>\n",
       "      <td>PL 1822/2019</td>\n",
       "      <td>Altera a Lei nº 11.340, de 7 de agosto de 2006...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Altera a Lei nº 11.340, de 7 de agosto de\\n200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57107</th>\n",
       "      <td>2358877</td>\n",
       "      <td>PL</td>\n",
       "      <td>2019</td>\n",
       "      <td>3815</td>\n",
       "      <td>PL 3815/2019</td>\n",
       "      <td>Altera a Lei nº 7.565, de 19 de dezembro de 19...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Altera a Lei nº 7.565, de 19 de dezembro\\nde  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57108</th>\n",
       "      <td>2358878</td>\n",
       "      <td>PL</td>\n",
       "      <td>2019</td>\n",
       "      <td>3130</td>\n",
       "      <td>PL 3130/2019</td>\n",
       "      <td>Altera a Lei nº 13.675, de 11 de junho de 2018...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Altera a Lei nº 13.675, de 11 de junho de\\n201...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57109 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       codProposicao txtSiglaTipo  numAno  numNumero       txtNome  \\\n",
       "0              16357   PL            1999       1165  PL 1165/1999   \n",
       "1              19098   PL            1992       3097  PL 3097/1992   \n",
       "2              20464   PL            2000       3927  PL 3927/2000   \n",
       "3              20683   PL            1998       4117  PL 4117/1998   \n",
       "4              20857   PL            1998       4395  PL 4395/1998   \n",
       "...              ...          ...     ...        ...           ...   \n",
       "57104        2358873   PL            2023       2233  PL 2233/2023   \n",
       "57105        2358874   PL            2019       3616  PL 3616/2019   \n",
       "57106        2358875   PL            2019       1822  PL 1822/2019   \n",
       "57107        2358877   PL            2019       3815  PL 3815/2019   \n",
       "57108        2358878   PL            2019       3130  PL 3130/2019   \n",
       "\n",
       "                                               txtEmenta  \\\n",
       "0      Altera dispositivo da Lei nº 8.987, de 13 de f...   \n",
       "1      Dispõe sobre a eleição de diretores de fundos ...   \n",
       "2      Altera a composição dos Tribunais Regionais do...   \n",
       "3      Dispõe sobre o acesso a ambientes de uso colet...   \n",
       "4      Estabelece as Diretrizes Nacionais de Defesa C...   \n",
       "...                                                  ...   \n",
       "57104  Altera o parágrafo único do art. 71 do Código ...   \n",
       "57105  Altera a Lei nº 9.503, de 23 de setembro de 19...   \n",
       "57106  Altera a Lei nº 11.340, de 7 de agosto de 2006...   \n",
       "57107  Altera a Lei nº 7.565, de 19 de dezembro de 19...   \n",
       "57108  Altera a Lei nº 13.675, de 11 de junho de 2018...   \n",
       "\n",
       "                                     txtExplicacaoEmenta  \\\n",
       "0      Estabelece que as concessionárias disponibiliz...   \n",
       "1                                                    NaN   \n",
       "2      Altera a composição do TRT da 5ª região, 6ª re...   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "57104                                                NaN   \n",
       "57105                                                NaN   \n",
       "57106                                                NaN   \n",
       "57107                                                NaN   \n",
       "57108                                                NaN   \n",
       "\n",
       "                                            txtIndexacao  \\\n",
       "0      Alteração, Lei das Concessões de Serviços Públ...   \n",
       "1      NORMAS, ELEIÇÃO DIRETA, EMPREGADO, APOSENTADO,...   \n",
       "2      Alteração, Lei Federal, composição, Tribunal R...   \n",
       "3      Autorização, pessoa portadora de deficiência, ...   \n",
       "4      Fixação, diretrizes, defesa civil, (Sindec), d...   \n",
       "...                                                  ...   \n",
       "57104                                                NaN   \n",
       "57105                                                NaN   \n",
       "57106                                                NaN   \n",
       "57107                                                NaN   \n",
       "57108                                                NaN   \n",
       "\n",
       "                                          txtInteiroTeor  \n",
       "0      Ofício nº 1416  (SF)                          ...  \n",
       "1      COMISSÃO DE CONSTITUIÇÃO E JUSTIÇA E DE REDAÇÃ...  \n",
       "2      COMISSÃO DE TRABALHO, DE ADMINISTRAÇÃO E SERVI...  \n",
       "3      Câmara dos Deputados\\n           Departamento ...  \n",
       "4      - 1 - \\nCOMISSÃO DE CONSTITUIÇÃO E JUSTIÇA E D...  \n",
       "...                                                  ...  \n",
       "57104  1 \\n  \\n \\n \\n \\nCÂMARA DOS DEPUTADOS \\nPROJET...  \n",
       "57105  Altera a Lei nº 9.503, de 23 de setembro\\nde 1...  \n",
       "57106  Altera a Lei nº 11.340, de 7 de agosto de\\n200...  \n",
       "57107  Altera a Lei nº 7.565, de 19 de dezembro\\nde  ...  \n",
       "57108  Altera a Lei nº 13.675, de 11 de junho de\\n201...  \n",
       "\n",
       "[57109 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d704396a",
   "metadata": {},
   "source": [
    "## Sem pré processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "103b42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf9023",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083fc915",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={'txtInteiroTeor': 'contents'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbf268e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"sem_pre_processamento/sem_pre_processamento.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a8e24",
   "metadata": {},
   "source": [
    "## Letra minuscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7938926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    terms = word_tokenize(txt.lower())\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e97a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69a7cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8c227a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"lower/pre_processamento_lower.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c11ad",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0085566",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"proposicao-tema-completo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "404a804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26956c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"imgArquivoTeorPDF\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be92cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69b5b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"imgArquivoTeorPDF\"] = dados1[\"imgArquivoTeorPDF\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d318e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"lower_remocao_punctuation/pre_processamento_lower_remocao_punctuation.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f343349",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação e remoção de acentuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89fbacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"proposicao-tema-completo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97b95d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bbf5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"imgArquivoTeorPDF\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c73b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9508cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c57bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"imgArquivoTeorPDF\"] = dados1[\"imgArquivoTeorPDF\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f5cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a0bc1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"lower_remocao_punctuation_acentuacao/pre_processamento_lower_remocao_punctuation_acentuacao.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f3d73",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "498a94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b630f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c496976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0885490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1b4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d8e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bd130e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"contents\"] = dados1[\"contents\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca01616",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb2662a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 142.550391\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4d039a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"lower_remocao_punctuation_acentuacao_stopwords/pre_processamento_lower_remocao_punctuation_acentuacao_stopwords.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97336b74",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335847e",
   "metadata": {},
   "source": [
    "## Stemming (RSLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d340da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c855147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "755b05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e59dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    stemmer = RSLPStemmer()\n",
    "    terms = word_tokenize(txt)\n",
    "    terms = [stemmer.stem(word) for word in terms]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfeb21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffa8b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"rslp/pre_processamento_rslp.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40311a",
   "metadata": {},
   "source": [
    "## Stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12544ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a3caea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa571334",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb475daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Savoy:\n",
    "\n",
    "    def __removeAllPTAccent(self, old_word):\n",
    "        word = list(old_word)\n",
    "        len_word = len(word)-1\n",
    "        for i in range(len_word, -1, -1):\n",
    "            if word[i] == 'ä':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'â':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'à':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'á':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ã':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ê':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'é':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'è':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ë':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ï':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'î':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ì':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'í':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ü':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ú':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ù':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'û':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ô':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ö':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ó':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ò':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'õ':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ç':\n",
    "                word[i] = 'c'\n",
    "\n",
    "        new_word = \"\".join(word)\n",
    "        return new_word\n",
    "\n",
    "    def __finalVowelPortuguese(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 'e' or word[-1] == 'a' or word[-1] == 'o':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __remove_PTsuffix(self, word):\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and (word[-3] == 'r' or word[-3] == 's' or word[-3] == 'z' or word[-3] == 'l'):\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if (word[-1] == 's' and word[-2] == 'i') and (word[-3] == 'e' or word[-3] == 'é'):\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'ó':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                return sing\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'õ':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'ã':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-1] == 'e' and word[-2] == 't' and word[-3] == 'n' and word[-4] == 'e' and word[-5] == 'm':\n",
    "                word = word[:-5]\n",
    "                return word\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __normFemininPortuguese(self, word):\n",
    "\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word < 3 or word[-1] != 'a':\n",
    "            return word\n",
    "\n",
    "        if len_word > 6:\n",
    "\n",
    "            if word[-2] == 'h' and word[-3] == 'n' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'a' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'i' and word[-4] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-2] == 'n' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'o':\n",
    "                word = word[:-1]\n",
    "                return word\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ê'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'v' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'm' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 2:\n",
    "            word = self.__remove_PTsuffix(word)\n",
    "            word = self.__normFemininPortuguese(word)\n",
    "            word = self.__finalVowelPortuguese(word)\n",
    "            word = self.__removeAllPTAccent(word)\n",
    "\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85e90f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    stemmer = Savoy()\n",
    "    terms = word_tokenize(txt)\n",
    "    terms = [stemmer.stem(word) for word in terms]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "670d8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "373ca92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"savoy/pre_processamento_savoy.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afbfbe6",
   "metadata": {},
   "source": [
    "## Stemming (RSLP-S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8181f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee2b3068",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a1027ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b5c5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSLP_S:\n",
    "    def __plural_reduction(self, word):\n",
    "        excep = [\"lápis\",\"cais\",\"mais\",\"crúcis\",\"biquínis\",\"pois\",\"depois\",\"dois\",\"leis\" ]\n",
    "        excep_s = [\"aliás\",\"pires\",\"lápis\",\"cais\",\"mais\",\"mas\",\"menos\", \"férias\",\"fezes\",\"pêsames\",\"crúcis\",\"gás\", \"atrás\",\"moisés\",\"através\",\"convés\",\"ês\", \"país\",\"após\",\"ambas\",\"ambos\",\"messias\"]\n",
    "\n",
    "        len_word = len(word)\n",
    "        new_word = list(word)\n",
    "\n",
    "        if len_word >= 3:\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'n':\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'õ':\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return  sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'ã':\n",
    "                if word == 'mães':\n",
    "                    word = word[:-1]\n",
    "                    return word\n",
    "                else:\n",
    "                    new_word[-2] = 'o'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    sing = sing[:-1]\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'a':\n",
    "                if word != 'cais' and word != 'mais':\n",
    "                    new_word[-2] = 'l'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    sing = sing[:-1]\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'é':\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'e':\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'ó':\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i':\n",
    "                if word not in excep:\n",
    "                    new_word[-1] = 'l'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'l':\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'r':\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "\n",
    "            if new_word[-1] == 's':\n",
    "                if word not in excep_s:\n",
    "                    word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        word = self.__plural_reduction(word)\n",
    "\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "342da31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    stemmer = RSLP_S()\n",
    "    terms = word_tokenize(txt)\n",
    "    terms = [stemmer.stem(word) for word in terms]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "451f75df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27e39c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"rslps/pre_processamento_rslps.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce468c",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9280ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e60f8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ad9f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)\n",
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8893580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c36b43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2385f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"contents\"] = dados1[\"contents\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eecfe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2910ec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 1202.042559\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51f13c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"rslp_full/pre_processamento_rslp_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6773aaec",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57b25bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0aa08730",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5acc76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)\n",
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5353629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6712ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "756a637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"contents\"] = dados1[\"contents\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ec86785",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d193544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 340.090966\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1276ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"savoy_full/pre_processamento_savoy_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4754f",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLP-S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "759819e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3f82e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d0f02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a2994a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05beaed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35412a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff90731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e54d967f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 185.940062\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7908188",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"rslps_full/pre_processamento_rslps_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f6b4b",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce84750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7e35183",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "52623ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9ad02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    terms = word_tokenize(txt)\n",
    "    ngram = []\n",
    "    \n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"#\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0228e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ae6d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "133c6be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"bigram/pre_processamento_bigram.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863f78c",
   "metadata": {},
   "source": [
    "## Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "00982bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f664a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8cfcfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d15e6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    terms = word_tokenize(txt)\n",
    "    ngram = []\n",
    "    \n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \"#\" + w[1] + \"#\" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d180061",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f9aba511",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "047108eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"trigram/pre_processamento_trigram.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52342b6b",
   "metadata": {},
   "source": [
    "## Unigram + Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e7e57482",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ec929d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ee1760fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee27bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    terms = word_tokenize(txt)\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"#\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8d977b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8cc476e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9521a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"unigram_bigram/pre_processamento_unigram_bigram.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ede03f",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e40b2",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5cbfcbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e65edb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7f502c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "72683b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"#\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c7d6e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ab8fb70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d7a5e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"bigram_full/pre_processamento_bigram_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0aff94",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "58c5ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "53f530b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4306a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c63ed24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \"#\" + w[1] + \"#\" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e6d87efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0ba572a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6c1edcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"trigram_full/pre_processamento_trigram_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b305d5a",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dd396b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "24537c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fe2e177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ba95ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"#\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d079abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f648f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c83f3aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"unigram_bigram_full/pre_processamento_unigram_bigram_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67b784",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + rslp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d056f",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + bigram (rslp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a034582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74dbdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "\n",
    "    ngram = []\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "\n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "\n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1325cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0d45eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"bigram_rslp_full/pre_processamento_bigram__rslp_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd976c39",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + trigram (rslp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9bcb80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c91bce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3b15480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8ec1a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \"#\" + w[1] + \"#\" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c661cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9589ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "62f86027",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"trigram_rslp_full/pre_processamento_trigram_rslp_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61d656",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + unigram + bigram (rslp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c0119e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f141f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9056053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)\n",
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a21e143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f562c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "781ee3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"contents\"] = dados1[\"contents\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f754b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92fed848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 1069.067130\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1af91ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"unigram_bigram_rslp_full/pre_processamento_unigram_bigram_rslp_full1.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b55ad0",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + Savoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f403cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Savoy:\n",
    "\n",
    "    def __removeAllPTAccent(self, old_word):\n",
    "        word = list(old_word)\n",
    "        len_word = len(word)-1\n",
    "        for i in range(len_word, -1, -1):\n",
    "            if word[i] == 'ä':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'â':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'à':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'á':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ã':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ê':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'é':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'è':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ë':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ï':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'î':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ì':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'í':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ü':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ú':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ù':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'û':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ô':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ö':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ó':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ò':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'õ':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ç':\n",
    "                word[i] = 'c'\n",
    "\n",
    "        new_word = \"\".join(word)\n",
    "        return new_word\n",
    "\n",
    "    def __finalVowelPortuguese(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 'e' or word[-1] == 'a' or word[-1] == 'o':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __remove_PTsuffix(self, word):\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and (word[-3] == 'r' or word[-3] == 's' or word[-3] == 'z' or word[-3] == 'l'):\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if (word[-1] == 's' and word[-2] == 'i') and (word[-3] == 'e' or word[-3] == 'é'):\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'ó':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                return sing\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'õ':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'ã':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-1] == 'e' and word[-2] == 't' and word[-3] == 'n' and word[-4] == 'e' and word[-5] == 'm':\n",
    "                word = word[:-5]\n",
    "                return word\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __normFemininPortuguese(self, word):\n",
    "\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word < 3 or word[-1] != 'a':\n",
    "            return word\n",
    "\n",
    "        if len_word > 6:\n",
    "\n",
    "            if word[-2] == 'h' and word[-3] == 'n' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'a' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'i' and word[-4] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-2] == 'n' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'o':\n",
    "                word = word[:-1]\n",
    "                return word\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ê'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'v' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'm' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 2:\n",
    "            word = self.__remove_PTsuffix(word)\n",
    "            word = self.__normFemininPortuguese(word)\n",
    "            word = self.__finalVowelPortuguese(word)\n",
    "            word = self.__removeAllPTAccent(word)\n",
    "\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651e2fb",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + bigram (savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7521b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "059bdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c33e0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f0750beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "\n",
    "    ngram = []\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "\n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"#\" + w[1]\n",
    "        ngram.append(string)\n",
    "\n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a3991545",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4a8e9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7010cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"bigram_savoy_full/pre_processamento_bigram__savoy_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7de52f",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + trigram (savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7b40c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "60b4ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "331a5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "cd6aa6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \"#\" + w[1] + \"#\" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0b14c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7c178243",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2bf53e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"trigram_savoy_full/pre_processamento_trigram_savoy_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9907b296",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + unigram + bigram (savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f09d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f264dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33c2f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "adc1f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc5b067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ed31730",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28ed128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "254d934f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 371.950056\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5af199ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94183a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"unigram_bigram_savoy_full/pre_processamento_unigram_bigram_savoy_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e8c75",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + RSLP-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e23719",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71260d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e70d0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9127fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "\n",
    "    ngram = []\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "\n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"#\" + w[1]\n",
    "        ngram.append(string)\n",
    "\n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d2bbc4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "71edf2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0323b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"bigram_rslps_full/pre_processamento_bigram__rslps_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa466c74",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + trigram (RSLPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8e6812b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "68e81a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "73fa98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a4f6660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \"#\" + w[1] + \"#\" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1316c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c0e018fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "35bd45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"trigram_rslps_full/pre_processamento_trigram_rslps_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdbb4f4",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + unigram + bigram (RSLPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2ba132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4fed1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9740a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtInteiroTeor\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "031bbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "436affaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d1c49540",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtInteiroTeor\"] = dados1[\"txtInteiroTeor\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95f26dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1e37738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 215.609079\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b25db949",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtInteiroTeor\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6fafbc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"unigram_bigram_rslps_full/pre_processamento_unigram_bigram_rslps_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcc71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
