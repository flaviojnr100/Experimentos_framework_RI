{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790501b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import Dict, Union, Any, List, Optional, Tuple\n",
    "\n",
    "from haystack import MultiLabel, Document\n",
    "from haystack.pipelines import Pipeline\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.nodes import BM25Retriever\n",
    "from haystack.nodes.base import BaseComponent\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from unicodedata import normalize\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce8c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_query = \"dados-conle-anonimizado-assunto-notnull (1).csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d9e0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_nome = \"experimento_word_n_gram\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1da4b4",
   "metadata": {},
   "source": [
    "# Base de dados câmara dos deputados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c647da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assunto= pd.read_csv(caminho_query, encoding='utf-8', delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43732db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_assunto = df_assunto.to_numpy()\n",
    "y,X = arr_assunto[:,0],arr_assunto[:,1]\n",
    "y = [i.strip() for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83fac529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar(y,top_n):\n",
    "    if y in top_n:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25bca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliacaoRecall(isPreprocess):\n",
    "    \n",
    "    \n",
    "    quant_encontrado=0\n",
    "    quant_relevante =0\n",
    "    for l,x in zip(y,X):\n",
    "        \n",
    "        tokenized_query3 = x                   \n",
    "        if isPreprocess:\n",
    "            tokenized_query3 = preprocess(x)                   \n",
    "    \n",
    "    \n",
    "        top_n_stem_l = pipeline.run(query=tokenized_query3,params={\"Retriever\": {\"top_k\": 20}})\n",
    "    \n",
    "\n",
    "        top_n = [top_n_stem_l['documents'][d].meta['name'].strip() for d in range(len(top_n_stem_l['documents']))]              #L\n",
    "    \n",
    "        quant_relevante+=1\n",
    "        quant_encontrado+=verificar(l,top_n)\n",
    "    \n",
    "    recall = quant_encontrado / quant_relevante\n",
    "    print(\"R@20: \"+str(recall))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944da510",
   "metadata": {},
   "source": [
    "# Pré processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19166942",
   "metadata": {},
   "source": [
    "## 1- Sem pré processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fc7539d",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "076f8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cdb17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97648462",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea1ba0",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad3ffded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5423728813559322\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c963108",
   "metadata": {},
   "source": [
    "## 2- Letra mínuscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca3ef6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a13a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = \" \".join(terms)\n",
    "        return terms\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d173db7",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_lowercase\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e04a9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "823bfea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43361040",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c5e1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fdf9aa",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40263378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5288135593220339\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a632d4",
   "metadata": {},
   "source": [
    "## 3- Letra mínuscula + remoção de pontuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7321ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    stopwords = list(punctuation)\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7ac639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        stopwords = list(punctuation)\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [word for word in terms if word not in stopwords]\n",
    "        terms = \" \".join(terms)\n",
    "        return terms\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67b164db",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_lowercase_pontuacao\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b5e1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e474acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "220d9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8886fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c25b6c",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3534c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5288135593220339\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c2ea7",
   "metadata": {},
   "source": [
    "## 4- Letra mínuscula + remoção de pontuação e remoção de acentuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2215f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f58661cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [word for word in terms if word not in stopwords]\n",
    "        terms = \" \".join(terms)\n",
    "        return terms\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3199a38",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_lowercase_pontuacao_acentuacao\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "805c75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b2b5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd3a976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9233873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9d3d6",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71d7e909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5932203389830508\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c868b",
   "metadata": {},
   "source": [
    "## 5- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45f91e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c56d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [word for word in terms if word not in stopwords]\n",
    "        terms = \" \".join(terms)\n",
    "        return terms\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9723e665",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_lowercase_pontuacao_acentuacao_stopword\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f728a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d8ab8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da86abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4aaa5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e4b91",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e7bbc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5898305084745763\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7de317",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88d729",
   "metadata": {},
   "source": [
    "## 6- Stemming (RSLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a2518f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt)\n",
    "    terms = [stemmer.stem(word) for word in terms]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9438ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt)\n",
    "        terms = [stemmer.stem(word) for word in terms]\n",
    "        terms = \" \".join(terms)\n",
    "        return terms\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cceed5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"text_rslp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e453eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eb74cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "878908de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ebe3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dab89e",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e732d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5084745762711864\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00099533",
   "metadata": {},
   "source": [
    "## 7- Stemming (RSLP-S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5854aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSLP_S:\n",
    "    def __plural_reduction(self, word):\n",
    "        excep = [\"lápis\",\"cais\",\"mais\",\"crúcis\",\"biquínis\",\"pois\",\"depois\",\"dois\",\"leis\" ]\n",
    "        excep_s = [\"aliás\",\"pires\",\"lápis\",\"cais\",\"mais\",\"mas\",\"menos\", \"férias\",\"fezes\",\"pêsames\",\"crúcis\",\"gás\", \"atrás\",\"moisés\",\"através\",\"convés\",\"ês\", \"país\",\"após\",\"ambas\",\"ambos\",\"messias\"]\n",
    "\n",
    "        len_word = len(word)\n",
    "        new_word = list(word)\n",
    "\n",
    "        if len_word >= 3:\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'n':\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'õ':\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return  sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'ã':\n",
    "                if word == 'mães':\n",
    "                    word = word[:-1]\n",
    "                    return word\n",
    "                else:\n",
    "                    new_word[-2] = 'o'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    sing = sing[:-1]\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'a':\n",
    "                if word != 'cais' and word != 'mais':\n",
    "                    new_word[-2] = 'l'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    sing = sing[:-1]\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'é':\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'e':\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'ó':\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i':\n",
    "                if word not in excep:\n",
    "                    new_word[-1] = 'l'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'l':\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'r':\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "\n",
    "            if new_word[-1] == 's':\n",
    "                if word not in excep_s:\n",
    "                    word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        word = self.__plural_reduction(word)\n",
    "\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15732163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt)\n",
    "    terms = [stemmer.stem(word) for word in terms]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b05f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt)\n",
    "        terms = [stemmer.stem(word) for word in terms]\n",
    "        terms = \" \".join(terms)\n",
    "        return terms\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25ac5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"text_rslps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be393e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c970783",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b2c32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c12c81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb22dc26",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4642c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5288135593220339\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587fec55",
   "metadata": {},
   "source": [
    "## 8- Stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a2cef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Savoy:\n",
    "\n",
    "    def __removeAllPTAccent(self, old_word):\n",
    "        word = list(old_word)\n",
    "        len_word = len(word)-1\n",
    "        for i in range(len_word, -1, -1):\n",
    "            if word[i] == 'ä':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'â':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'à':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'á':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ã':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ê':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'é':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'è':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ë':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ï':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'î':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ì':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'í':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ü':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ú':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ù':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'û':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ô':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ö':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ó':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ò':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'õ':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ç':\n",
    "                word[i] = 'c'\n",
    "\n",
    "        new_word = \"\".join(word)\n",
    "        return new_word\n",
    "\n",
    "    def __finalVowelPortuguese(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 'e' or word[-1] == 'a' or word[-1] == 'o':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __remove_PTsuffix(self, word):\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and (word[-3] == 'r' or word[-3] == 's' or word[-3] == 'z' or word[-3] == 'l'):\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if (word[-1] == 's' and word[-2] == 'i') and (word[-3] == 'e' or word[-3] == 'é'):\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'ó':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                return sing\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'õ':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'ã':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-1] == 'e' and word[-2] == 't' and word[-3] == 'n' and word[-4] == 'e' and word[-5] == 'm':\n",
    "                word = word[:-5]\n",
    "                return word\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __normFemininPortuguese(self, word):\n",
    "\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word < 3 or word[-1] != 'a':\n",
    "            return word\n",
    "\n",
    "        if len_word > 6:\n",
    "\n",
    "            if word[-2] == 'h' and word[-3] == 'n' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'a' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'i' and word[-4] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-2] == 'n' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'o':\n",
    "                word = word[:-1]\n",
    "                return word\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ê'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'v' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'm' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 2:\n",
    "            word = self.__remove_PTsuffix(word)\n",
    "            word = self.__normFemininPortuguese(word)\n",
    "            word = self.__finalVowelPortuguese(word)\n",
    "            word = self.__removeAllPTAccent(word)\n",
    "\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91e04f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt)\n",
    "    terms = [stemmer.stem(word) for word in terms]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d067a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt)\n",
    "        terms = [stemmer.stem(word) for word in terms]\n",
    "        terms = \" \".join(terms)\n",
    "        return terms\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84b47f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"text_savoy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa3be743",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b78d2407",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba6feac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63d3fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc20656",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "601ab1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5084745762711864\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3416b7",
   "metadata": {},
   "source": [
    "## 9- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f35f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed5cc837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc6ebc83",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_rslp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97a0ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f33ca15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b80ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "011b262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf160a12",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "702975bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5457627118644067\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafff496",
   "metadata": {},
   "source": [
    "## 10- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLP-S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39453aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32977cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1f89b79",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_rslps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0feaf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3064b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cd51669",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29aad401",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192080cc",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e019f6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5898305084745763\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723b9b5c",
   "metadata": {},
   "source": [
    "## 11- Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c1e4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fd74207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "        stopwords.extend(list(punctuation))\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "        return \" \".join(terms)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95940543",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_savoy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6c58a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43232224",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6373a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a57d0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d6245",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b65137bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5830508474576271\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41278532",
   "metadata": {},
   "source": [
    "# Word n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7107c1",
   "metadata": {},
   "source": [
    "## 12- Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df1318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt=str(txt)\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt)\n",
    "    ngram = []\n",
    "    \n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc81792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def preprocess(self,txt):\n",
    "        txt=str(txt)\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt)\n",
    "        ngram = []\n",
    "    \n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \" \" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "662ead5f",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"text_bigram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae69fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cccf0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acf34675",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5578d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbe2d3",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dd3283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5084745762711864\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101fa4a9",
   "metadata": {},
   "source": [
    "## 13- Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5db3757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt=str(txt)\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt)\n",
    "    ngram = []\n",
    "    \n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3981b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def preprocess(self,txt):\n",
    "        txt=str(txt)\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt)\n",
    "        ngram = []\n",
    "    \n",
    "        ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "        for w in ngram_3:\n",
    "            string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "709c2405",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"text_trigram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d1237ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ea92f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e65cca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ab0d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c435def",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b8974e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.4711864406779661\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8336b",
   "metadata": {},
   "source": [
    "## 14- Unigram + Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f39abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt=str(txt)\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt)\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "898471d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def preprocess(self,txt):\n",
    "        txt=str(txt)\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt)\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \" \" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5078de7",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"text_uni_bi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f709b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa09a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f45041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc3ac01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6315475",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d329dee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.4847457627118644\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79cd81",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb4448",
   "metadata": {},
   "source": [
    "## 15- Letra mínuscula + remoção de pontuação, acentuação e stopword + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104efa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69ca475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [word for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \" \" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60353cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_bigram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf31ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1ebe573",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e068bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb4a2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ccf722",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30793b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5627118644067797\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101634d",
   "metadata": {},
   "source": [
    "## 16- Letra mínuscula + remoção de pontuação, acentuação e stopword + trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d80b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e1eb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [word for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "        for w in ngram_3:\n",
    "            string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0892fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_trigram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daddcd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aef9b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efd5afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1fe8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940359a9",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bba0deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5322033898305085\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb9d1b",
   "metadata": {},
   "source": [
    "## 17- Letra mínuscula + remoção de pontuação, acentuação e stopword + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb4a4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85516487",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [word for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \" \" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a29161b4",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_uni_bi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f6f25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8dafb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4453979",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4507e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b52ef",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11e2e55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5389830508474577\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760fe05f",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + RSLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3802f025",
   "metadata": {},
   "source": [
    "## 18- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP) + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8a0423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cb22466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \" \" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55cd47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_bigram_rslp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f1094ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f00dc606",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d15cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0abf783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de56ead",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0062628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.4745762711864407\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c23d65",
   "metadata": {},
   "source": [
    "## 19- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP) + trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3edfdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4159973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "        for w in ngram_3:\n",
    "            string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5832cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_trigram_rslp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bafa46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0daec2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4cb3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecd5d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c492e13",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d82bf3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.4542372881355932\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f9f64",
   "metadata": {},
   "source": [
    "## 20- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04344d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a939b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "\n",
    "        stemmer = RSLPStemmer()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \" \" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c42f683",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_uni_bi_rslp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cda5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88d38eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a56257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e2a64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c2022",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c870f0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.46440677966101696\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267136bc",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + RSLP-S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3340b0",
   "metadata": {},
   "source": [
    "## 21- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP-S) + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189c1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ef398b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \" \" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "    \n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02aa9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_bigram_rslps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3e3f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28271f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a5338bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ca1a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37059de",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41c47e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5559322033898305\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c43df4",
   "metadata": {},
   "source": [
    "## 22- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP-S) + trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e9ed076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83b0b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "        for w in ngram_3:\n",
    "            string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e717aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_trigram_rslps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a06ff89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28e00616",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bab4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50521304",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f026f",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b845a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5322033898305085\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989eee7",
   "metadata": {},
   "source": [
    "## 23- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (RSLP-S) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a55552f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c45866f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "\n",
    "        stemmer = RSLP_S()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \" \" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce42f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_uni_bi_rslps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4d3dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca2c7ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c6a8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a28accca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673662e",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32173748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5423728813559322\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c46c1c",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + Savoy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de868ad3",
   "metadata": {},
   "source": [
    "## 24- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (Savoy) + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ad05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894c6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \" \" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e03e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_bigram_savoy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70170f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26f9b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e6b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2798cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a0ebec",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a7630a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5322033898305085\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3dd5be",
   "metadata": {},
   "source": [
    "## 25- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (Savoy) + trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9ca7628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "    for w in ngram_3:\n",
    "        string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10f17fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_3 = list(ngrams(terms, 3))\n",
    "        \n",
    "        for w in ngram_3:\n",
    "            string = w[0] + \" \" + w[1] + \" \" + w[2]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bf27427",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_trigram_savoy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d15d2be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc7af0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdff55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4252661",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9ddb2",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76fd3f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5152542372881356\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3815b42",
   "metadata": {},
   "source": [
    "## 26- Letra mínuscula + remoção de pontuação, acentuação e stopword + stemming (Savoy) + unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76f9b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = list(punctuation)\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \" \" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35b58bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessamento(BaseComponent):\n",
    "    outgoing_edges = 1\n",
    "    \n",
    "    def _remove_acentos(self,txt):\n",
    "        return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    def preprocess(self,txt):\n",
    "        txt = self._remove_acentos(txt)\n",
    "        stopwords = list(punctuation)\n",
    "\n",
    "        stemmer = Savoy()\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        terms = tokenizer.tokenize(txt.lower())\n",
    "        terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "        ngram = []\n",
    "        ngram_1 = list(ngrams(terms, 1))\n",
    "        ngram_2 = list(ngrams(terms, 2))\n",
    "        for w in ngram_1:\n",
    "            ngram.append(w[0])\n",
    "        \n",
    "        for w in ngram_2:\n",
    "            string = w[0] + \" \" + w[1]\n",
    "            ngram.append(string)\n",
    "    \n",
    "        return \" \".join(ngram)\n",
    "      \n",
    "    def run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[MultiLabel] = None,\n",
    "        documents: Optional[List[Document]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "    ) -> Tuple[Dict, str]:\n",
    "        query = self.preprocess(query)\n",
    "        output = {\"query\": query}\n",
    "        return output, \"output_1\"\n",
    "\n",
    "    def run_batch(\n",
    "        self,\n",
    "        queries: Optional[Union[str, List[str]]] = None,\n",
    "        file_paths: Optional[List[str]] = None,\n",
    "        labels: Optional[Union[MultiLabel, List[MultiLabel]]] = None,\n",
    "        documents: Optional[Union[List[Document], List[List[Document]]]] = None,\n",
    "        meta: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "        params: Optional[dict] = None,\n",
    "        debug: Optional[bool] = None,\n",
    "    ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7762ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    " document_store = ElasticsearchDocumentStore(host='localhost', port=9200, username='elastic',\n",
    "                                                password='_3egIk1UEsLOV4266NWo', index=index_nome,\n",
    "                                                search_fields=[\"pre_text_uni_bi_savoy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11c460ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6735014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = PreProcessamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b865f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb4e14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_node(component=pre, name=\"Pre\",inputs=['Query'])\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\",inputs=['Pre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9e151",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b727c8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@20: 0.5423728813559322\n"
     ]
    }
   ],
   "source": [
    "avaliacaoRecall(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45859525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
