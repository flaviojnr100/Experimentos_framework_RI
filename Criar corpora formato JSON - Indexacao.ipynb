{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0cf84a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Flavio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Flavio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\Flavio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from unicodedata import normalize\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('rslp')\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50615748",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6187de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc027dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d704396a",
   "metadata": {},
   "source": [
    "## Sem pré processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103b42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtIndexacao\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ecf9023",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "083fc915",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={'txtIndexacao': 'contents'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbf268e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"sem_pre_processamento_indexacao/sem_pre_processamento.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f3d73",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "498a94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b630f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c496976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtIndexacao\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0885490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtIndexacao\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d1b4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [word for word in terms if word not in stopwords]\n",
    "    terms = \" \".join(terms)\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78cfb1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd130e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"contents\"] = dados1[\"contents\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f93a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f4eda4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 20.870001\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d039a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"lower_remocao_punctuation_acentuacao_stopwords_indexacao/pre_processamento_lower_remocao_punctuation_acentuacao_stopwords.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb475daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Savoy:\n",
    "\n",
    "    def __removeAllPTAccent(self, old_word):\n",
    "        word = list(old_word)\n",
    "        len_word = len(word)-1\n",
    "        for i in range(len_word, -1, -1):\n",
    "            if word[i] == 'ä':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'â':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'à':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'á':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ã':\n",
    "                word[i] = 'a'\n",
    "            if word[i] == 'ê':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'é':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'è':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ë':\n",
    "                word[i] = 'e'\n",
    "            if word[i] == 'ï':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'î':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ì':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'í':\n",
    "                word[i] = 'i'\n",
    "            if word[i] == 'ü':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ú':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ù':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'û':\n",
    "                word[i] = 'u'\n",
    "            if word[i] == 'ô':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ö':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ó':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ò':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'õ':\n",
    "                word[i] = 'o'\n",
    "            if word[i] == 'ç':\n",
    "                word[i] = 'c'\n",
    "\n",
    "        new_word = \"\".join(word)\n",
    "        return new_word\n",
    "\n",
    "    def __finalVowelPortuguese(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 'e' or word[-1] == 'a' or word[-1] == 'o':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __remove_PTsuffix(self, word):\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and (word[-3] == 'r' or word[-3] == 's' or word[-3] == 'z' or word[-3] == 'l'):\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if (word[-1] == 's' and word[-2] == 'i') and (word[-3] == 'e' or word[-3] == 'é'):\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i' and word[-3] == 'ó':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 3:\n",
    "            if word[-1] == 's' and word[-2] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                return sing\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'õ':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "            if word[-1] == 's' and word[-2] == 'e' and word[-3] == 'ã':\n",
    "                new_word = list(word)\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-1] == 'e' and word[-2] == 't' and word[-3] == 'n' and word[-4] == 'e' and word[-5] == 'm':\n",
    "                word = word[:-5]\n",
    "                return word\n",
    "\n",
    "        if len_word > 2:\n",
    "            if word[-1] == 's':\n",
    "                word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __normFemininPortuguese(self, word):\n",
    "\n",
    "        len_word = len(word)\n",
    "\n",
    "        if len_word < 3 or word[-1] != 'a':\n",
    "            return word\n",
    "\n",
    "        if len_word > 6:\n",
    "\n",
    "            if word[-2] == 'h' and word[-3] == 'n' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'a' and word[-4] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'i' and word[-4] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        if len_word > 5:\n",
    "            if word[-2] == 'n' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'r' and word[-3] == 'o':\n",
    "                word = word[:-1]\n",
    "                return word\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'o':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 's' and word[-3] == 'e':\n",
    "                new_word = list(word)\n",
    "                new_word[-3] = 'ê'\n",
    "                masc = \"\".join(new_word)\n",
    "                masc = masc[:-1]\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'c' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'd' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'v' and word[-3] == 'i':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'm' and word[-3] == 'a':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "            if word[-2] == 'n':\n",
    "                new_word = list(word)\n",
    "                new_word[-1] = 'o'\n",
    "                masc = \"\".join(new_word)\n",
    "                return masc\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        len_word = len(word)\n",
    "        if len_word > 2:\n",
    "            word = self.__remove_PTsuffix(word)\n",
    "            word = self.__normFemininPortuguese(word)\n",
    "            word = self.__finalVowelPortuguese(word)\n",
    "            word = self.__removeAllPTAccent(word)\n",
    "\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce468c",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9280ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e60f8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ad9f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtIndexacao\"]]\n",
    "dados1 = dados.assign(id=colId)\n",
    "dados1 = dados1.rename(columns={\"txtIndexacao\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8893580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd46c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0598840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"contents\"] = dados1[\"contents\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad201a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1328feb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 102.515673\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51f13c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"rslp_full_indexacao/pre_processamento_rslp_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6773aaec",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (Savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57b25bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aa08730",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5acc76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtIndexacao\"]]\n",
    "dados1 = dados.assign(id=colId)\n",
    "dados1 = dados1.rename(columns={\"txtIndexacao\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5353629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1c429ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3a1f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"contents\"] = dados1[\"contents\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fd21125",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23137086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 34.434150\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1276ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"savoy_full_indexacao/pre_processamento_savoy_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67b784",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + rslp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61d656",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + unigram + bigram (rslp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c0119e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f141f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9056053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtIndexacao\"]]\n",
    "dados1 = dados.assign(id=colId)\n",
    "dados1 = dados1.rename(columns={\"txtIndexacao\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a21e143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f21595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46e25853",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"contents\"] = dados1[\"contents\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "370b81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f08eda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 116.638859\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1af91ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"unigram_bigram_rslp_full_indexacao/pre_processamento_unigram_bigram_rslp_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b55ad0",
   "metadata": {},
   "source": [
    "# Word n-gram + pré processamento básico + Savoy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9907b296",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + unigram + bigram (savoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f09d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f264dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33c2f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtIndexacao\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adc1f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = Savoy()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1159d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "015553bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtIndexacao\"] = dados1[\"txtIndexacao\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ef13610",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3799ac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 41.625165\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5af199ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtIndexacao\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94183a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"unigram_bigram_savoy_full_indexacao/pre_processamento_unigram_bigram_savoy_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884b9d6",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação + remoção de acentuação e remoção de stopword + stemming (RSLP_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ac64fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSLP_S:\n",
    "    def __plural_reduction(self, word):\n",
    "        excep = [\"lápis\",\"cais\",\"mais\",\"crúcis\",\"biquínis\",\"pois\",\"depois\",\"dois\",\"leis\" ]\n",
    "        excep_s = [\"aliás\",\"pires\",\"lápis\",\"cais\",\"mais\",\"mas\",\"menos\", \"férias\",\"fezes\",\"pêsames\",\"crúcis\",\"gás\", \"atrás\",\"moisés\",\"através\",\"convés\",\"ês\", \"país\",\"após\",\"ambas\",\"ambos\",\"messias\"]\n",
    "\n",
    "        len_word = len(word)\n",
    "        new_word = list(word)\n",
    "\n",
    "        if len_word >= 3:\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'n':\n",
    "                new_word[-2] = 'm'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'õ':\n",
    "                new_word[-3] = 'ã'\n",
    "                new_word[-2] = 'o'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return  sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'ã':\n",
    "                if word == 'mães':\n",
    "                    word = word[:-1]\n",
    "                    return word\n",
    "                else:\n",
    "                    new_word[-2] = 'o'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    sing = sing[:-1]\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'a':\n",
    "                if word != 'cais' and word != 'mais':\n",
    "                    new_word[-2] = 'l'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    sing = sing[:-1]\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'é':\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'e':\n",
    "                new_word[-3] = 'e'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i' and new_word[-3] == 'ó':\n",
    "                new_word[-3] = 'o'\n",
    "                new_word[-2] = 'l'\n",
    "                sing = \"\".join(new_word)\n",
    "                sing = sing[:-1]\n",
    "                return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'i':\n",
    "                if word not in excep:\n",
    "                    new_word[-1] = 'l'\n",
    "                    sing = \"\".join(new_word)\n",
    "                    return sing\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'l':\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "\n",
    "            if new_word[-1] == 's' and new_word[-2] == 'e' and new_word[-3] == 'r':\n",
    "                word = word[:-2]\n",
    "                return word\n",
    "\n",
    "            if new_word[-1] == 's':\n",
    "                if word not in excep_s:\n",
    "                    word = word[:-1]\n",
    "\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        word = self.__plural_reduction(word)\n",
    "\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "445aa17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91991270",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9a34642",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtIndexacao\"]]\n",
    "dados1 = dados.assign(id=colId)\n",
    "dados1 = dados1.rename(columns={\"txtIndexacao\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "122748c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "        \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e30ef478",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6f35a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"contents\"] = dados1[\"contents\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "daf2ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2b54d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 27.356000\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a42baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"rslps_full_indexacao/pre_processamento_rslps_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df024c6",
   "metadata": {},
   "source": [
    "## Letra mínuscula + remoção de pontuação, acentuação e stopword + unigram + bigram (RSLP_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "70a55ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"base_20230428_douglas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebb8a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colId = range(len(dados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3b1f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados[[\"txtNome\",\"txtIndexacao\"]]\n",
    "dados1 = dados.assign(id=colId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6f2abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = str(txt)\n",
    "    txt = _remove_acentos(txt)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    stopwords.extend(list(punctuation))\n",
    "\n",
    "    stemmer = RSLP_S()\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    terms = tokenizer.tokenize(txt.lower())\n",
    "    terms = [stemmer.stem(word) for word in terms if word not in stopwords]\n",
    "    \n",
    "    ngram = []\n",
    "    ngram_1 = list(ngrams(terms, 1))\n",
    "    ngram_2 = list(ngrams(terms, 2))\n",
    "    for w in ngram_1:\n",
    "        ngram.append(w[0])\n",
    "        \n",
    "    for w in ngram_2:\n",
    "        string = w[0] + \"_\" + w[1]\n",
    "        ngram.append(string)\n",
    "    \n",
    "    return \" \".join(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36998d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2fc351b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1[\"txtIndexacao\"] = dados1[\"txtIndexacao\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c63b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10b9399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração: 34.258001\n"
     ]
    }
   ],
   "source": [
    "print(\"Duração: %f\" %(d-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6b4c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1 = dados1.rename(columns={\"txtIndexacao\":\"contents\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1901846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados1.to_json(\"unigram_bigram_rslps_full_indexacao/pre_processamento_unigram_bigram_rslps_full.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac987fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
